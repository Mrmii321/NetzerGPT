import os
import json
from openai import OpenAI
from openai import AssistantEventHandler
from dotenv import load_dotenv
from typing_extensions import override  # Import override

load_dotenv(".env")


class EventHandler(AssistantEventHandler):
    """
    Handles events for the OpenAI assistant responses.
    
    Inherits from AssistantEventHandler to manage response streaming.
    
    Attributes:
        formatted_response (str): Holds the full response from the assistant.
    """

    def __init__(self):
        """
        Initializes the EventHandler and sets up the formatted_response.
        """
        super().__init__()  # Call the superclass initializer
        self.formatted_response = ""

    @override  # Use the override decorator
    def on_text_created(self, text) -> None:
        """
        Event triggered when the assistant creates text.
        
        Args:
            text (str): The text created by the assistant.
        """
        print(f"\nassistant > ", end="\n", flush=True)

    @override
    def on_text_delta(self, delta, snapshot):
        """
        Event triggered for each delta of text generated by the assistant.
        
        Args:
            delta (object): The delta object containing the text change.
            snapshot (object): The snapshot of the current state of the conversation.
        """
        print(delta.value, end="", flush=True)
        self.formatted_response += delta.value

    # Additional methods can be implemented if needed


class Main:
    """
    The main class for interacting with the AI.

    Attributes:
        client (OpenAI): The OpenAI client for API interaction.
        model (str): The model identifier for the AI.
        json_data (dict): The configuration loaded from a JSON file.
        assistant_id (str): The ID of the assistant to interact with.
        thread (object): The conversation thread created for interaction.
    """
    
    def __init__(self):
        """
        Initializes the Main class, sets up the OpenAI client, and loads configuration.
        """
        api_key = os.getenv("OPENAI_API_KEY")
        self.client = OpenAI(api_key=api_key)
        self.model = "ft:gpt-4o-2024-08-06:personal:netzergpt:AIs8J7vW"  # Corrected model name

        # Load configuration
        with open(r"C:\Users\noahf\OneDrive\Desktop\Development\Python\NetzerGPT\config.json", "r") as file:
            self.json_data = json.load(file)

        # Use your existing Assistant ID or create a new assistant
        self.assistant_id = "asst_djxKX8cZQZHPVNtqbb9GBGE1"  # Replace with your Assistant ID

        # Create a new thread (conversation)
        self.thread = self.client.beta.threads.create()


    def process_response(self, response):
        """
        Processes the assistant's response to replace annotations with inline source citations.

        This method extracts the content of the response and iterates over any annotations that provide
        citations. It replaces the annotated text with either the quoted text from the file or a reference
        to the file path, avoiding the use of special characters or placeholder citation formats.

        Args:
            response (object): The response object from the assistant that includes text content and annotations.

        Returns:
            tuple: 
                - message_content (str): The assistant's response with inline citations replacing annotations.
                - citations (list): A list of citations extracted from the annotations (if any).
        
        Example:
            If the response contains annotated text that refers to a quote from a file, the method will replace
            the annotated text with the actual quote and file name, e.g., "The sky is blue from source_file.txt".
        """
        # Extract the message content
        message_content = response.content[0].text
        annotations = response.annotations
        citations = []

        # Iterate over the annotations and replace them with inline citations
        for index, annotation in enumerate(annotations):
            # Inject the actual source text instead of using weird characters
            if (file_citation := getattr(annotation, 'file_citation', None)):
                cited_file = self.client.files.retrieve(file_citation.file_id)
                source_text = f'{file_citation.quote} from {cited_file.filename}'
                message_content = message_content.replace(annotation.text, source_text)

            elif (file_path := getattr(annotation, 'file_path', None)):
                # Handle file_path annotations if file_citation is not present
                source_text = f'Referenced from file at {file_path}'
                message_content = message_content.replace(annotation.text, source_text)
            
            else:
                # If there is no valid citation information, remove weird characters
                message_content = message_content.replace(annotation.text, "")

        return message_content



    def get_response(self, message: str, max_tokens: int = 150) -> str:
        """
        Generates a response from the AI based on the provided message.

        If the message is "/dump", it fetches all messages in the thread.
        Otherwise, it sends the user message to the assistant and streams the response.

        Args:
            message (str): The user message to send to the assistant.
            max_tokens (int): The maximum number of tokens for the response (default is 150).
        Returns:
            str: The formatted response from the assistant or an error message.
        """
        try:
            if message == "/dump":
                # Fetch all messages in the thread
                messages = self.client.beta.threads.messages.list(thread_id=self.thread.id)
                formatted_response = json.dumps([msg.to_dict() for msg in messages], indent=2)
            else:
                self.client.beta.threads.messages.create(
                    thread_id=self.thread.id,
                    role="user",
                    content=message
                )

                # Create an EventHandler instance
                event_handler = EventHandler()

                # Start streaming the assistant's response using event_handler
                with self.client.beta.threads.runs.stream(
                    thread_id=self.thread.id,
                    assistant_id=self.assistant_id,
                    instructions=' '.join(self.json_data["prompts"]["system_prompt"]),
                    event_handler=event_handler,
                ) as stream:
                    stream.until_done()

                # Get the full response from the event_handler
                response = event_handler.formatted_response

                # Process the response to handle annotations
                formatted_response, citations = self.process_response(response)

            return formatted_response

        except Exception as e:
            return f"An error occurred: {str(e)}"

